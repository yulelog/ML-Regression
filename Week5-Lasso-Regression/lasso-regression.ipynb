{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "distinguished-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import log, sqrt\n",
    "from sklearn import linear_model  # using scikit-learn\n",
    "import numpy as np\n",
    "\n",
    "dtype_dict = {'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, 'grade':int, 'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, 'long':float, 'sqft_lot15':float, 'sqft_living':float, 'floors':float, 'condition':int, 'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int}\n",
    "sales = pd.read_csv('kc_house_data.csv', dtype=dtype_dict)\n",
    "testing = pd.read_csv('wk3_kc_house_test_data.csv', dtype=dtype_dict)\n",
    "training = pd.read_csv('wk3_kc_house_train_data.csv', dtype=dtype_dict)\n",
    "validation = pd.read_csv('wk3_kc_house_valid_data.csv', dtype=dtype_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "seven-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df['sqft_living_sqrt'] = df['sqft_living'].apply(sqrt)\n",
    "    df['sqft_lot_sqrt'] = df['sqft_lot'].apply(sqrt)\n",
    "    df['bedrooms_square'] = df['bedrooms']*df['bedrooms']\n",
    "    df['floors_square'] = df['floors']*df['floors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "awful-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add features to all data sets\n",
    "for df in [sales, testing, training, validation]:\n",
    "    add_features(df)\n",
    "    \n",
    "all_features = [\n",
    "    'bedrooms', 'bedrooms_square',\n",
    "    'bathrooms',\n",
    "    'sqft_living', 'sqft_living_sqrt',\n",
    "    'sqft_lot', 'sqft_lot_sqrt',\n",
    "    'floors', 'floors_square',\n",
    "    'waterfront', 'view', 'condition', 'grade',\n",
    "    'sqft_above',\n",
    "    'sqft_basement',\n",
    "    'yr_built', 'yr_renovated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "rational-percentage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sqft_living', 'view', 'grade']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso with L1 = 5e2\n",
    "model_all = linear_model.Lasso(alpha=5e2, normalize=True) # set parameters\n",
    "model_all.fit(sales[all_features], sales['price']) # learn weights\n",
    "# Features chosen by LASSO\n",
    "[all_features[c] for c in [i for i, x in enumerate(model_all.coef_ !=0) if x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "compliant-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find a good L1 penalty value using a validation set\n",
    "l1_penalties = np.logspace(1, 7, num=13)\n",
    "results = []\n",
    "for l1 in l1_penalties:\n",
    "    # Learn a model on TRAINING data using the specified l1_penalty\n",
    "    model = linear_model.Lasso(alpha=l1, normalize=True)\n",
    "    model.fit(training[all_features], training['price'])\n",
    "    # Compute the RSS on VALIDATION for the current model\n",
    "    rss = sum((validation['price'] - model.predict(validation[all_features]))**2)\n",
    "    results.append({'l1': l1, 'rss': rss})\n",
    "\n",
    "pd.DataFrame(results).sort_values('rss').iloc[0]['l1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "neural-cosmetic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RSS:  98467402552698.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the RSS on TEST data for the model with the best L1 penalty selected above with lowest RSS\n",
    "final_model = linear_model.Lasso(alpha=10, normalize=True)\n",
    "final_model.fit(training[all_features], training['price'])\n",
    "rss = sum((testing['price'] - final_model.predict(testing[all_features]))**2)\n",
    "print('Test RSS: ', rss)\n",
    "\n",
    "# Using the best L1 penalty, how many nonzero weights do you have? \n",
    "# Count the number of nonzero coefficients first, and add 1 if the intercept is also nonzero\n",
    "np.count_nonzero(final_model.coef_) + np.count_nonzero(final_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-natural",
   "metadata": {},
   "source": [
    "#### Limiting the model to a set number of features #### \n",
    "- Explore a large range of ‘l1_penalty’ values to find a narrow region of ‘l1_penalty’ values where models are likely to have the desired number of non-zero weights.\n",
    "- Further explore the narrow region you found to find a good value for ‘l1_penalty’ that achieves the desired sparsity.  Here, we will again use a validation set to choose the best value for ‘l1_penalty’.\n",
    "\n",
    "Out of this large range, we want to find the two ends of our desired narrow range of l1_penalty. At one end, we will have l1_penalty values that have too few non-zeros, and at the other end, we will have an l1_penalty that has too many non-zeros.\n",
    "\n",
    "More formally, find:\n",
    "- The largest l1_penalty that has more non-zeros than ‘max_nonzeros’ (if we pick a penalty smaller than this value, we will definitely have too many non-zero weights)Store this value in the variable ‘l1_penalty_min’ (we will use it later)\n",
    "- The smallest l1_penalty that has fewer non-zeros than ‘max_nonzeros’ (if we pick a penalty larger than this value, we will definitely have too few non-zero weights)Store this value in the variable ‘l1_penalty_max’ (we will use it later)\n",
    "\n",
    "Hint: there are many ways to do this, e.g.:\n",
    "- Programmatically within the loop above\n",
    "- Creating a list with the number of non-zeros for each value of l1_penalty and inspecting it to find the appropriate boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "educational-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nonzeros = 7\n",
    "\n",
    "more_nonzeros = []\n",
    "fewer_nonzeros = []\n",
    "\n",
    "for l1 in np.logspace(1, 4, num=20):\n",
    "    # Fit a regression model with a given l1_penalty on TRAIN data\n",
    "    model = linear_model.Lasso(alpha=l1, normalize=True)\n",
    "    model.fit(training[all_features], training['price'])\n",
    "    count_nonzeros = np.count_nonzero(model.coef_) + np.count_nonzero(model.intercept_)\n",
    "    if count_nonzeros > max_nonzeros:\n",
    "        more_nonzeros.append(l1)\n",
    "    if count_nonzeros < max_nonzeros:\n",
    "        fewer_nonzeros.append(l1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "atmospheric-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_penalty_min = max(more_nonzeros)\n",
    "l1_penalty_max = min(fewer_nonzeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-spotlight",
   "metadata": {},
   "source": [
    "Exploring narrower range of l1_penalty between ‘l1_penalty_min’ and ‘l1_penalty_max’:\n",
    "\n",
    "We look for the L1 penalty in this range that produces exactly the right number of nonzeros and also minimizes RSS on the VALIDATION set.\n",
    "\n",
    "For l1_penalty in np.linspace(l1_penalty_min,l1_penalty_max,20):\n",
    "- Fit a regression model with a given l1_penalty on TRAIN data. As before, use \"alpha=l1_penalty\" and \"normalize=True\".\n",
    "Measure the RSS of the learned model on the VALIDATION set\n",
    "- Find the model that the lowest RSS on the VALIDATION set and has sparsity equal to ‘max_nonzeros’. (Again, take account of the intercept when counting the number of nonzeros.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "liable-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for l1 in np.linspace(l1_penalty_min,l1_penalty_max,20):\n",
    "    # Fit a regression model with a given l1_penalty on TRAIN data\n",
    "    model = linear_model.Lasso(alpha=l1, normalize=True)\n",
    "    model.fit(training[all_features], training['price'])\n",
    "    # Measure the RSS of the learned model on the VALIDATION set\n",
    "    rss = sum((validation['price'] - model.predict(validation[all_features]))**2)\n",
    "    # Find the model that the lowest RSS on the VALIDATION set and has sparsity equal to ‘max_nonzeros’.\n",
    "    count_nonzeros = np.count_nonzero(model.coef_) + np.count_nonzero(model.intercept_)\n",
    "    \n",
    "    result.append({'l1': l1, 'rss': rss, 'count_nonzeros': count_nonzeros})\n",
    "\n",
    "\n",
    "result = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "subtle-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1_penalty value in our narrow range which has the lowest RSS on the VALIDATION set and has sparsity equal to ‘max_nonzeros’\n",
    "selected_l1 = result[result['count_nonzeros']==max_nonzeros].sort_values('rss').reset_index(drop=True).iloc[0]['l1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "therapeutic-field",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bathrooms', 'sqft_living', 'waterfront', 'view', 'grade', 'yr_built']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_selected_l1 = linear_model.Lasso(alpha=selected_l1, normalize=True)\n",
    "model_with_selected_l1.fit(training[all_features], training['price'])\n",
    "[all_features[c] for c in [i for i, x in enumerate(model_with_selected_l1.coef_ !=0) if x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-shell",
   "metadata": {},
   "source": [
    "### Implement LASSO solver via coordinate descent ###\n",
    "\n",
    "- Write a function to normalize features\n",
    "- Implement coordinate descent for LASSO\n",
    "- Explore effects of L1 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "spectacular-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(df, features, output):\n",
    "    df['constant'] = 1 \n",
    "    features = ['constant'] + features\n",
    "    features_df = df[features]\n",
    "    \n",
    "    feature_matrix = features_df.to_numpy()\n",
    "    output_array = df['price'].to_numpy()\n",
    "\n",
    "    return feature_matrix, output_array\n",
    "\n",
    "def predict_output(feature_matrix, weights):\n",
    "    predictions = np.dot(feature_matrix, weights)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-letters",
   "metadata": {},
   "source": [
    "In the house dataset, features vary wildly in their relative magnitude: ‘sqft_living’ is very large overall compared to ‘bedrooms’, for instance. As a result, weight for ‘sqft_living’ would be much smaller than weight for ‘bedrooms’. This is problematic because “small” weights are dropped first as l1_penalty goes up.\n",
    "\n",
    "To give equal considerations for all features, we need to normalize features as discussed in the lectures: we divide each feature by its 2-norm so that the transformed feature has norm 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "blind-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features):\n",
    "    \"\"\"\n",
    "    normalize columns of a given feature matrix.\n",
    "    :param features: columns of a feature matrix\n",
    "    :return normalized features: normalized columns of given feature matrix\n",
    "    :return norms: norms of orignal features\n",
    "    \"\"\"\n",
    "    \n",
    "    norms = np.linalg.norm(features, axis=0)\n",
    "    normalized_features = features / norms\n",
    "    \n",
    "    return normalized_features, norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-custody",
   "metadata": {},
   "source": [
    "#### Coordinate Descent: ####\n",
    "\n",
    "At each iteration, we will fix all weights but weight i and find the value of weight i that minimizes the objective. \n",
    "\n",
    "All weights other than w[i] are held to be constant. We will optimize one w[i] at a time, circling through the weights multiple times.\n",
    "- Pick a coordinate i\n",
    "- Compute w[i] that minimizes the LASSO cost function\n",
    "- Repeat the two steps for all coordinates, multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-police",
   "metadata": {},
   "source": [
    "#### Effect of L1 penalty ####\n",
    "\n",
    "Consider a simple model with 2 features: ‘sqft_living’ and ‘bedrooms’. The output is ‘price’.\n",
    "\n",
    "1. First, run get_numpy_data() (or equivalent) to obtain a feature matrix with 3 columns (constant column added). Use the entire ‘sales’ dataset for now.\n",
    "2. Normalize columns of the feature matrix. Save the norms of original features as ‘norms’.\n",
    "3. Set initial weights to [1,4,1].\n",
    "4. Make predictions with feature matrix and initial weights.\n",
    "5. Compute values of ro[i], where ro[i] = SUM[ [feature_i]*(output - prediction + w[i]*[feature_i]) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "exterior-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, output_array = get_numpy_data(sales, ['sqft_living', 'bedrooms'], 'price')\n",
    "normalized_features, norms = normalize_features(feature_matrix)\n",
    "initial_weights = [1,4,1]\n",
    "predictions = predict_output(normalized_features, initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "amended-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_1 = sum(normalized_features[:,1] * (sales['price'] - predictions + initial_weights[1] * normalized_features[:,1]))\n",
    "ro_2 = sum(normalized_features[:,2] * (sales['price'] - predictions + initial_weights[2] * normalized_features[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-guitar",
   "metadata": {},
   "source": [
    "Range of l1_penalty values that would not set w[1] zero, but would set w[2] zero\n",
    "- -l1_penalty/2 <= ro_2 <= l1_penalty/2\n",
    "- ro_1 > l1_penalty / 2\n",
    "\n",
    "Equivalent to: 2*ro_2 <= l1_penalty < 2*ro_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "seven-geometry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161933397.3324781  < l1_penalty <  175878941.64650303\n"
     ]
    }
   ],
   "source": [
    "# Range of l1_penalty values that would not set w[1] zero, but would set w[2] zero\n",
    "print(ro_2*2, \" < l1_penalty < \", ro_1*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-child",
   "metadata": {},
   "source": [
    "Range of l1_penalty values that would set both w[1] and w[2] to zero\n",
    "- -l1_penalty/2 <= ro_1 <= l1_penalty/2\n",
    "- -l1_penalty/2 <= ro_2 <= l1_penalty/2\n",
    "\n",
    "Equivalent to: l1_penalty >= 2*ro_1 (because ro_1 > ro_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "precious-session",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1_penalty >  175878941.64650303\n"
     ]
    }
   ],
   "source": [
    "# Range of l1_penalty values that would set both w[1] and w[2] zero\n",
    "print(\"l1_penalty > \", ro_1*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-experiment",
   "metadata": {},
   "source": [
    "=> So we can say that ro[i] quantifies the significance of the i-th feature: the larger ro[i] is, the more likely it is for the i-th feature to be retained.\n",
    "\n",
    "\n",
    "#### Single Coordinate Descent Step ####\n",
    "Using the formula above, implement coordinate descent that minimizes the cost function over a single feature i. Note that the intercept (weight 0) is not regularized. The function should accept feature matrix, output, current weights, l1 penalty, and index of feature to optimize over. The function should return new weight for feature i.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "powered-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_coordinate_descent_step(i, feature_matrix, output, weights, l1_penalty):\n",
    "    # compute prediction\n",
    "    predictions = predict_output(feature_matrix, weights)\n",
    "    # compute ro[i] = SUM[ [feature_i]*(output - prediction + weight[i]*[feature_i]) ]\n",
    "    ro_i = sum(feature_matrix[:,i] * (output - predictions + weights[i] * feature_matrix[:,i]))\n",
    "    \n",
    "    if i == 0: # intercept -- do not regularize\n",
    "        new_weight_i = ro_i\n",
    "    elif ro_i < -l1_penalty/2.:\n",
    "        new_weight_i = ro_i + l1_penalty/2.\n",
    "    elif ro_i > l1_penalty/2.:\n",
    "        new_weight_i = ro_i - l1_penalty/2.\n",
    "    else:\n",
    "        new_weight_i = 0.\n",
    "    \n",
    "    return new_weight_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "colonial-courage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4255588466910251\n"
     ]
    }
   ],
   "source": [
    "# TEST: should print 0.425558846691\n",
    "print(lasso_coordinate_descent_step(1, np.array([[3./sqrt(13),1./sqrt(10)],\n",
    "                   [2./sqrt(13),3./sqrt(10)]]), np.array([1., 1.]), np.array([1., 4.]), 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-school",
   "metadata": {},
   "source": [
    "#### Cyclical coordinate descent ####\n",
    "\n",
    "Now that we have a function that optimizes the cost function over a single coordinate, let us implement cyclical coordinate descent where we optimize coordinates 0, 1, ..., (d-1) in order and repeat.\n",
    "\n",
    "When do we know to stop? Each time we scan all the coordinates (features) once, we measure the change in weight for each coordinate. If no coordinate changes by more than a specified threshold, we stop.\n",
    "\n",
    "For each iteration:\n",
    "- As you loop over features in order and perform coordinate descent, measure how much each coordinate changes.\n",
    "- After the loop, if the maximum change across all coordinates is falls below the tolerance, stop. Otherwise, go back to the previous step.\n",
    "Return weights\n",
    "\n",
    "The function should accept the following parameters:\n",
    "- Feature matrix\n",
    "- Output array\n",
    "- Initial weights\n",
    "- L1 penalty\n",
    "- Tolerance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "collaborative-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_cyclical_coordinate_descent(feature_matrix, output, initial_weights, l1_penalty, tolerance):\n",
    "    # initialize variables\n",
    "    weights = np.array(initial_weights)\n",
    "    max_step = tolerance\n",
    "    \n",
    "    while max_step >= tolerance:\n",
    "        step_sizes = []\n",
    "        for i in range(len(weights)):\n",
    "            new_weight = lasso_coordinate_descent_step(i, feature_matrix, output, weights, l1_penalty)\n",
    "            step_size = new_weight - weights[i]\n",
    "            weights[i] = new_weight\n",
    "            step_sizes.append(step_size)\n",
    "        \n",
    "        max_step = max(step_sizes)\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-found",
   "metadata": {},
   "source": [
    "Let us now go back to the simple model with 2 features: ‘sqft_living’ and ‘bedrooms’. Using ‘get_numpy_data’ (or equivalent), extract the feature matrix and the output array from from the house dataframe. Then normalize the feature matrix using ‘normalized_features()’ function.\n",
    "\n",
    "Using the following parameters, learn the weights on the sales dataset:\n",
    "- Initial weights = all zeros\n",
    "- L1 penalty = 1e7\n",
    "- Tolerance = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "responsible-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, output_array = get_numpy_data(sales, ['sqft_living', 'bedrooms'], 'price')\n",
    "normalized_features, norms = normalize_features(feature_matrix)\n",
    "initial_weights = [0, 0, 0]\n",
    "l1_penalty = 1e7\n",
    "tolerance = 1\n",
    "\n",
    "simple_model_weights = lasso_cyclical_coordinate_descent(normalized_features, output_array, initial_weights, l1_penalty, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "guilty-spyware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.630492e+15'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RSS of the learned model on the normalized dataset\n",
    "\"{:e}\".format(sum((output_array - predict_output(normalized_features, simple_model_weights))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "human-convert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21624995, 63157249,        0])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which features had weight zero at convergence?\n",
    "simple_model_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-letter",
   "metadata": {},
   "source": [
    "#### Evaluating LASSO fit with more features ####\n",
    "\n",
    "Create a normalized feature matrix from the TRAINING data with the following set of features: bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition, grade, sqft_above, sqft_basement, yr_built, yr_renovated\n",
    "\n",
    "First, learn the weights with l1_penalty=1e7, on the training data. Initialize weights to all zeros, and set the tolerance=1. Call resulting weights’ weights1e7’, you will need them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "jewish-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    " 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']\n",
    "\n",
    "feature_matrix, output_array = get_numpy_data(training, more_features, 'price')\n",
    "normalized_features, norms = normalize_features(feature_matrix)\n",
    "l1_penalty = 1e7\n",
    "initial_weights = np.zeros(len(more_features)+1)\n",
    "tolerance = 1\n",
    "weights1e7 = lasso_cyclical_coordinate_descent(normalized_features, output_array, initial_weights, l1_penalty, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "wireless-welsh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constant', 'sqft_living', 'waterfront', 'view']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which features had weight zero at convergence?\n",
    "[(['constant'] + more_features)[w] for w in [i for i, x in enumerate(weights1e7 !=0) if x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "restricted-ground",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constant']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_penalty=1e8\n",
    "weights1e8 = lasso_cyclical_coordinate_descent(normalized_features, output_array, initial_weights, l1_penalty, tolerance)\n",
    "[(['constant'] + more_features)[w] for w in [i for i, x in enumerate(weights1e8 !=0) if x]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "amazing-harbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constant',\n",
       " 'bedrooms',\n",
       " 'bathrooms',\n",
       " 'sqft_living',\n",
       " 'sqft_lot',\n",
       " 'floors',\n",
       " 'waterfront',\n",
       " 'view',\n",
       " 'condition',\n",
       " 'grade',\n",
       " 'sqft_above',\n",
       " 'sqft_basement',\n",
       " 'yr_built',\n",
       " 'yr_renovated']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_penalty = 1e4\n",
    "tolerance = 5e5\n",
    "weights1e4  = lasso_cyclical_coordinate_descent(normalized_features, output_array, initial_weights, l1_penalty, tolerance)\n",
    "[(['constant'] + more_features)[w] for w in [i for i, x in enumerate(weights1e4 !=0) if x]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-field",
   "metadata": {},
   "source": [
    "#### Rescaling learned weights ####\n",
    "\n",
    "Recall that we normalized our feature matrix, before learning the weights.  To use these weights on a test set, we must normalize the test data in the same way. Alternatively, we can rescale the learned weights to include the normalization, so we never have to worry about normalizing the test data: \n",
    "\n",
    "In this case, we must scale the resulting weights so that we can make predictions with original features:\n",
    "- Store the norms of the original features to a vector called ‘norms’: features, norms = normalize_features(features)\n",
    "- Run Lasso on the normalized features and obtain a ‘weights’ vector\n",
    "- Compute the weights for the original features by performing element-wise division, i.e. weights_normalized = weights / norms\n",
    "\n",
    "Now, we can apply weights_normalized to the test data, without normalizing it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "tested-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_weights1e4 = weights1e4 / norms\n",
    "normalized_weights1e7 = weights1e7 / norms\n",
    "normalized_weights1e8 = weights1e8 / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dense-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_matrix, test_output_array = get_numpy_data(testing, more_features, 'price')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "executive-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss1e4 = sum((test_output_array - predict_output(test_feature_matrix, normalized_weights1e4))**2)\n",
    "rss1e7 = sum((test_output_array - predict_output(test_feature_matrix, normalized_weights1e7))**2)\n",
    "rss1e8 = sum((test_output_array - predict_output(test_feature_matrix, normalized_weights1e8))**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "color-governor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.290853e+14\n",
      "1.631036e+14\n",
      "2.847189e+14\n"
     ]
    }
   ],
   "source": [
    "for rss in [rss1e4, rss1e7, rss1e8]:\n",
    "    print(\"{:e}\".format(rss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
