{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "identified-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "selected-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('kc_house_train_data.csv')\n",
    "test_data = pd.read_csv('kc_house_test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-inspector",
   "metadata": {},
   "source": [
    "## Assignment 1 ##\n",
    "\n",
    "\n",
    "### Feature Engineering ###\n",
    "\n",
    "Consider transformations of existing variables, e.g. the log of the square feet or even \"interaction\" variables such as the product of bedrooms and bathrooms. Add 4 new variables in both your train_data and test_data. \n",
    "\n",
    "- ‘bedrooms_squared’ = ‘bedrooms’*‘bedrooms’\n",
    "- ‘bed_bath_rooms’ = ‘bedrooms’*‘bathrooms’\n",
    "- ‘log_sqft_living’ = log(‘sqft_living’)\n",
    "- ‘lat_plus_long’ =  ‘lat’ + ‘long’\n",
    "\n",
    "Squaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4) since 1^2 = 1 but 4^2 = 16. Consequently this variable will mostly affect houses with many bedrooms.\n",
    "\n",
    "Bedrooms times bathrooms is what's called an \"interaction\" variable. It is large when both of them are large.\n",
    "\n",
    "Taking the log of square feet has the effect of bringing large values closer together and spreading out small values.\n",
    "\n",
    "Adding latitude to longitude is non-sensical but we will do it anyway (you'll see why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ordered-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_variables(df):\n",
    "    \"\"\"\n",
    "    add variables in place\n",
    "    :param df: pandas dataframe of the data set\n",
    "    \"\"\"\n",
    "    df['bedrooms_squared'] = df['bedrooms']*df['bedrooms']\n",
    "    df['bed_bath_rooms'] = df['bedrooms']*df['bathrooms']\n",
    "    df['log_sqft_living'] = np.log(df['sqft_living'])\n",
    "    df['lat_plus_long'] = df['lat'] + df['long']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continuing-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rss(y_actual, y_predict):\n",
    "    return sum((y_actual - y_predict)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "economic-cancer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bedrooms_squared    12.45\n",
       "bed_bath_rooms       7.50\n",
       "log_sqft_living      7.55\n",
       "lat_plus_long      -74.65\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add variables in place\n",
    "add_variables(train_data)\n",
    "add_variables(test_data)\n",
    "\n",
    "round(test_data[['bedrooms_squared', 'bed_bath_rooms', 'log_sqft_living', 'lat_plus_long']].apply(np.mean),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-conservation",
   "metadata": {},
   "source": [
    "### Regression Model ###\n",
    "\n",
    "- Model 1: ‘sqft_living’, ‘bedrooms’, ‘bathrooms’, ‘lat’, and ‘long’\n",
    "- Model 2: ‘sqft_living’, ‘bedrooms’, ‘bathrooms’, ‘lat’,‘long’, and ‘bed_bath_rooms’\n",
    "- Model 3: ‘sqft_living’, ‘bedrooms’, ‘bathrooms’, ‘lat’,‘long’, ‘bed_bath_rooms’, ‘bedrooms_squared’, ‘log_sqft_living’, and ‘lat_plus_long’\n",
    "(The three models here are “nested” in that all of the features of the Model 1 are in Model 2 and all of the features of Model 2 are in Model 3.)  \n",
    "\n",
    "Learn all three models on the TRAINING data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lucky-documentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.12258646e+02 -5.95865332e+04  1.57067421e+04  6.58619264e+05\n",
      " -3.09374351e+05]\n",
      "[ 3.06610053e+02 -1.13446368e+05 -7.14613083e+04  6.54844630e+05\n",
      " -2.94298969e+05  2.55796520e+04]\n",
      "[ 5.29422820e+02  3.45142296e+04  6.70607813e+04  5.34085611e+05\n",
      " -4.06750711e+05 -6.78858667e+03 -8.57050439e+03 -5.61831484e+05\n",
      "  1.27334900e+05]\n"
     ]
    }
   ],
   "source": [
    "model_1 = LinearRegression().fit(train_data[['sqft_living', 'bedrooms', 'bathrooms', 'lat', 'long']], train_data['price'])\n",
    "model_2 = LinearRegression().fit(train_data[['sqft_living', 'bedrooms', 'bathrooms', 'lat', 'long', 'bed_bath_rooms']], train_data['price'])\n",
    "model_3 = LinearRegression().fit(train_data[['sqft_living', 'bedrooms', 'bathrooms', 'lat', 'long', 'bedrooms_squared', 'bed_bath_rooms', 'log_sqft_living', 'lat_plus_long']], train_data['price'])\n",
    "\n",
    "print(model_1.coef_)\n",
    "print(model_2.coef_)\n",
    "print(model_3.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-virus",
   "metadata": {},
   "source": [
    "## Assignment 2 ##\n",
    "\n",
    "\n",
    "### Gradient Descent ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "egyptian-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(df, features, output):\n",
    "    \"\"\"\n",
    "    Takes a data set df, a list of features, and a output name, \n",
    "    return a 2D array of feature matrix and 1D array of output\n",
    "    :param df: pandas dataframe of the data set\n",
    "    :param features: list of the feature names to be selected\n",
    "    :param output: name of the output\n",
    "    :return features_matrix: a 2D features_matrix including an additional column of 1s as the constant feature\n",
    "    :return output_array: a 1D array of output\n",
    "    \"\"\"\n",
    "    # add a constant column\n",
    "    df['constant'] = 1 \n",
    "    # prepend variable 'constant' to the features list\n",
    "    features = ['constant'] + features\n",
    "    \n",
    "    # convert the features into a numpy matrix\n",
    "    features_matrix = df[features].to_numpy()\n",
    "    # convert the pandas Series into a numpy array\n",
    "    output_array = df[output].to_numpy()\n",
    "    output_array = output_array.reshape(output_array.shape[0],1)\n",
    "    \n",
    "    return features_matrix, output_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-command",
   "metadata": {},
   "source": [
    "If the features matrix (including a column of 1s for the constant) is stored as a 2D array (or matrix) and the regression weights are stored as a 1D array then the predicted output is just the dot product between the features matrix and the weights (with the weights on the right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "innovative-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_outcome(feature_matrix, weights):\n",
    "    \"\"\"\n",
    "    Predict outcome based on the feature matrix and weights given\n",
    "    :param feature_matrix: 2D matrix of features with a shape of N observations * D features (N*D)\n",
    "    :param weights: 1D array of weights with D features (D*1)\n",
    "    :return predictions: 1D array of N predicted outcome (N*1)\n",
    "    \"\"\"\n",
    "    predictions = np.dot(feature_matrix, weights)\n",
    "    predictions = predictions.reshape(predictions.shape[0],1)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-senate",
   "metadata": {},
   "source": [
    "If we have a the values of a single input feature in an array ‘feature’ and the prediction ‘errors’ (predictions - output) then the derivative of the regression cost function with respect to the weight of ‘feature’ is just twice the dot product between ‘feature’ and ‘errors’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "knowing-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    \"\"\"\n",
    "    Given an array of features for a single observation and prediction erros, \n",
    "    return the gradient of the feature (a single number)\n",
    "    :param errors: 1D array of errors (N*1)\n",
    "    :param feature: 1D array of weights for one feature for N observations (N*1)\n",
    "    :return : single number of derivative for the feature (1*1)\n",
    "    \"\"\"\n",
    "    return -2*np.dot(np.transpose(feature), errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-world",
   "metadata": {},
   "source": [
    "Although we can compute the derivative for all the features simultaneously (the gradient) we will explicitly loop over the features individually for simplicity. Write a gradient descent function that does the following:\n",
    "- Accepts a numpy feature_matrix 2D array, a 1D output array, an array of initial weights, a step size and a convergence tolerance.\n",
    "- While not converged updates each feature weight by subtracting the step size times the derivative for that feature given the current weights\n",
    "- At each step computes the magnitude/length of the gradient (square root of the sum of squared components)\n",
    "- When the magnitude of the gradient is smaller than the input tolerance returns the final weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "periodic-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    \"\"\"\n",
    "    Find weights for the regression model through gradient descent\n",
    "    :param feature_matrix: 2D matrix of features with a shape of N observations * D features (N*D)    \n",
    "    :param output: 1D array of actual output (N*1)\n",
    "    :param initial_weights: 1D array of initial weights (1*D)\n",
    "    :param step_size: number value given to the step size of gradient descent\n",
    "    :param tolerance: number value of the threshold when is coverged\n",
    "    :return weights: 1D array of final regression model weights\n",
    "    \"\"\"\n",
    "    converged = False\n",
    "    weights = np.array(initial_weights)\n",
    "    \n",
    "    while not converged:\n",
    "        # compute the predictions based on feature_matrix and weights \n",
    "        predictions = predict_outcome(feature_matrix, weights)  # shape: N*1\n",
    "\n",
    "        # compute the errors as predictions - output\n",
    "        errors = predictions - output  # shape: N*1\n",
    "\n",
    "        gradient_sum_squares = 0 # initialize the gradient\n",
    "        \n",
    "        # while not converged, update each weight individually\n",
    "        for i in range(len(weights)):\n",
    "            \n",
    "            # feature_matrix[:, i] is the feature column associated with weights[i]\n",
    "            # compute the derivative for weight[i]\n",
    "            derivative = feature_derivative(errors, feature_matrix[:, i])\n",
    "\n",
    "            # add the squared derivative to the gradient sum squares\n",
    "            gradient_sum_squares = gradient_sum_squares + derivative**2\n",
    "            \n",
    "            # update the weight based on step size and derivative:\n",
    "            weights[i] = weights[i]+step_size*derivative\n",
    "        \n",
    "        gradient_magnitude = sqrt(gradient_sum_squares)\n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "    return weights \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-mistress",
   "metadata": {},
   "source": [
    "#### Model 1 with only one feature: \"sqft_living\" ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eligible-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, output = get_numpy_data(train_data, ['sqft_living'], ['price'])\n",
    "initial_weights = [-47000., 1.]\n",
    "step_size = 7e-12 \n",
    "tolerance = 2.5e7\n",
    "model_1_weights = regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "metallic-oklahoma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-46999.88716555,    281.91211918])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "difficult-driving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price of the first house with model 1 weights:  [356134.443255]\n",
      "Model 1 RSS on test data:  [[2.75400045e+14]]\n"
     ]
    }
   ],
   "source": [
    "# Use model 1's weights to predict house prices on test data\n",
    "test_model_1_feature_matrix, test_output = get_numpy_data(test_data, ['sqft_living'], ['price'])\n",
    "model_1_test_predictions = predict_outcome(test_model_1_feature_matrix, model_1_weights)\n",
    "\n",
    "\n",
    "print(\"Predicted price of the first house with model 1 weights: \", model_1_test_predictions[0])\n",
    "\n",
    "model_1_rss = np.dot((test_output - model_1_test_predictions).T, (test_output - model_1_test_predictions)) \n",
    "print(\"Model 1 RSS on test data: \", model_1_rss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-village",
   "metadata": {},
   "source": [
    "#### Model 2 with two features: \"sqft_living\"  and \"sqft_living_15\" ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "elder-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, output = get_numpy_data(train_data, ['sqft_living', 'sqft_living15'], 'price')\n",
    "initial_weights = np.array([-100000., 1., 1.])\n",
    "step_size = 4e-12\n",
    "tolerance = 1e9\n",
    "model_2_weights = regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "choice-theology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price of the first house with model 1 weights:  [366651.41162949]\n",
      "Model 1 RSS on test data:  [[2.70263444e+14]]\n"
     ]
    }
   ],
   "source": [
    "test_model_2_feature_matrix, test_output = get_numpy_data(test_data, ['sqft_living', 'sqft_living15'], 'price')\n",
    "model_2_test_predictions = predict_outcome(test_model_2_feature_matrix, model_2_weights)\n",
    "\n",
    "print(\"Predicted price of the first house with model 1 weights: \", model_2_test_predictions[0])\n",
    "\n",
    "model_2_rss = np.dot((test_output - model_2_test_predictions).T, (test_output - model_2_test_predictions)) \n",
    "print(\"Model 1 RSS on test data: \", model_2_rss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
